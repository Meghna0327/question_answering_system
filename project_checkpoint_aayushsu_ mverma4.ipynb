{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import numpy as np\n",
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FAISS index path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAISS_INDEX_PATH = os.path.join(os.getcwd(), \"faiss_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load SQUAD2.0 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train', 'validation'])\n",
      "{'id': '5733be284776f41900661182', 'title': 'University_of_Notre_Dame', 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad = load_dataset(\"squad\")\n",
    "print(squad.keys())\n",
    "print(squad['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model Evaluations on SQUAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_squad_v2(model_name, num_samples=50):\n",
    "    print(f\"\\nEvaluating model: {model_name}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "    qa_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=\"mps\")\n",
    "\n",
    "    squad_dataset = load_dataset(\"squad_v2\", split=f\"validation[:{num_samples}]\")\n",
    "    squad_metric = evaluate.load(\"squad_v2\")\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for sample in squad_dataset:\n",
    "        context = sample[\"context\"]\n",
    "        question = sample[\"question\"]\n",
    "        answers = sample[\"answers\"]\n",
    "\n",
    "        prompt = f\"Context:\\n{context}\\n\\nQuestion:\\n{question}\\n\\nAnswer:\"\n",
    "        response = qa_pipeline(prompt, max_new_tokens=64)[0][\"generated_text\"]\n",
    "        predicted_answer = response.strip()\n",
    "\n",
    "        predictions.append({\n",
    "            \"id\": sample[\"id\"],\n",
    "            \"prediction_text\": predicted_answer,\n",
    "            \"no_answer_probability\": 0.0\n",
    "        })\n",
    "        references.append({\n",
    "            \"id\": sample[\"id\"],\n",
    "            \"answers\": answers\n",
    "        })\n",
    "\n",
    "    results = squad_metric.compute(predictions=predictions, references=references)\n",
    "    exact = results.get(\"exact_match\") or results.get(\"exact\")\n",
    "    f1 = results.get(\"f1\", 0.0)\n",
    "    print(f\"Exact Match (EM): {exact:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model: google/flan-t5-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match (EM): 38.00\n",
      "F1 Score: 39.90\n",
      "\n",
      "Evaluating model: declare-lab/flan-alpaca-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match (EM): 16.00\n",
      "F1 Score: 25.84\n",
      "\n",
      "Evaluating model: allenai/unifiedqa-t5-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match (EM): 26.00\n",
      "F1 Score: 32.33\n"
     ]
    }
   ],
   "source": [
    "baseline_models = [\n",
    "    \"google/flan-t5-base\",\n",
    "    \"declare-lab/flan-alpaca-base\",\n",
    "    \"allenai/unifiedqa-t5-base\"\n",
    "]\n",
    "\n",
    "results_model = {}\n",
    "\n",
    "for model_name in baseline_models:\n",
    "    results_model[model_name] = evaluate_model_on_squad_v2(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run evaluations on 50 samples, with the flan-t5-base model performing the best, as expected, since it's instruction-tuned on a wide variety of tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VectorDB setup and Embedding Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the files from the \"Document\" folder as texts and chunking them, which will later be stored as embeddings in the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_chunks_langchain(folder_name):\n",
    "    loader = DirectoryLoader(folder_name, glob=\"**/*.*\", loader_cls=TextLoader, recursive=True, use_multithreading=True)\n",
    "    sources = loader.load()\n",
    "    source_chunks = []\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=250)\n",
    "    source_chunks = splitter.split_documents(sources)\n",
    "    return source_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each chunk is vectorized into an embedding using the Sentence Transformer and then stored into the FAISS VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunks(chunks):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    result = FAISS.from_documents(chunks, embeddings)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunk the data, convert them into embeddings and persist the FAISS storage index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_data(folder_name):\n",
    "    chunks = get_text_chunks_langchain(folder_name)\n",
    "    db = process_chunks(chunks)\n",
    "    db.save_local(FAISS_INDEX_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a simple use-case we store and convert the final project questionairre pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting FAISS Path\n"
     ]
    }
   ],
   "source": [
    "persist_data(\"Documents/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retreival-Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = '''\n",
    "You are a helpful question-answering assistant. Given the following context, answer the question as accurately and concisely as possible.\n",
    "Context:\n",
    "{context}\n",
    "Question:\n",
    "{question}\n",
    "'''\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def prompt(question, model_name):\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    db = FAISS.load_local(FAISS_INDEX_PATH, embeddings, allow_dangerous_deserialization=True )\n",
    "\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": 4}, search_type=\"mmr\")\n",
    "\n",
    "    qaprompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=PROMPT)\n",
    "\n",
    "    model_id = model_name\n",
    "\n",
    "    pipe = pipeline(\n",
    "            task = \"text2text-generation\",\n",
    "            model = model_id,\n",
    "            top_p = 1,\n",
    "            do_sample = True,\n",
    "            temperature = 0.7,\n",
    "            max_length=512,\n",
    "        )\n",
    "\n",
    "    llm = HuggingFacePipeline(\n",
    "        pipeline=pipe,\n",
    "        batch_size=1,\n",
    "    )\n",
    "\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | qaprompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    return rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a RAG-chain, by passing the question from the user and getting the context from the vectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1041 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Alina Vereshchaka'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt(\"Who is the course instructor?\", \"google/flan-t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1041 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The course instructor is Alina Vereshchaka.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt(\"Who is the course instructor?\", \"declare-lab/flan-alpaca-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1040 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'project_checkpoint_TEAMMATE1_ TEAMMATE2.zip'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt(\"Who is the course instructor?\", \"allenai/unifiedqa-t5-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LegalBench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall utilized the below benchmark dataset to evaluate are legally instruction-tuned KG-based LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d906326312bc4468b878042d8b63ef3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data.tar.gz:   0%|          | 0.00/19.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4497ca6b8cc47c6a98cf96a083fd85f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64894b03c35142b89e8cb2a267491aa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/80 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "legalbench = load_dataset(\"nguha/legalbench\", \"contract_qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>index</th>\n",
       "      <th>question</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Does the clause discuss PII data breaches?</td>\n",
       "      <td>In the event of a data breach involving the un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Does the clause discuss dispute resolution?</td>\n",
       "      <td>In the event of any dispute arising out of or ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yes</td>\n",
       "      <td>2</td>\n",
       "      <td>Does the clause describe confidentiality requi...</td>\n",
       "      <td>Each party agrees to keep confidential and not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yes</td>\n",
       "      <td>3</td>\n",
       "      <td>Does the clause discuss choice of law governin...</td>\n",
       "      <td>This Agreement shall be governed by and constr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No</td>\n",
       "      <td>4</td>\n",
       "      <td>Does the clause waive confidentiality?</td>\n",
       "      <td>This Agreement shall be governed by and constr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  answer index                                           question  \\\n",
       "0    Yes     0         Does the clause discuss PII data breaches?   \n",
       "1    Yes     1        Does the clause discuss dispute resolution?   \n",
       "2    Yes     2  Does the clause describe confidentiality requi...   \n",
       "3    Yes     3  Does the clause discuss choice of law governin...   \n",
       "4     No     4             Does the clause waive confidentiality?   \n",
       "\n",
       "                                                text  \n",
       "0  In the event of a data breach involving the un...  \n",
       "1  In the event of any dispute arising out of or ...  \n",
       "2  Each party agrees to keep confidential and not...  \n",
       "3  This Agreement shall be governed by and constr...  \n",
       "4  This Agreement shall be governed by and constr...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "legalbench[\"train\"].to_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available splits: dict_keys(['train', 'test'])\n",
      "Column names: ['answer', 'index', 'question', 'text']\n",
      "Example sample:\n",
      " {'answer': 'Yes', 'index': '0', 'question': 'Does the clause discuss PII data breaches?', 'text': 'In the event of a data breach involving the unauthorized access, use, or disclosure of personally identifiable information (PII), the Company shall notify without undue delay affected individuals and relevant regulatory authorities in accordance with applicable laws and regulations. The Company shall also take reasonable steps to mitigate the harm caused by the breach and to prevent future breaches.'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Available splits:\", legalbench.keys())\n",
    "print(\"Column names:\", legalbench[\"train\"].column_names)\n",
    "print(\"Example sample:\\n\", legalbench[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "* LegalBench - https://hazyresearch.stanford.edu/legalbench/getting-started/\n",
    "* RAG - https://python.langchain.com/docs/tutorials/rag/\n",
    "* RAG 2 - https://python.langchain.com/docs/tutorials/qa_chat_history/\n",
    "* KG - https://python.langchain.com/docs/how_to/graph_constructing/\n",
    "* Graph DB - https://python.langchain.com/docs/how_to/graph_semantic/\n",
    "*  https://huggingface.co/declare-lab/flan-alpaca-base\n",
    "* FAISS - https://github.com/facebookresearch/faiss\n",
    "* https://www.anyscale.com/blog/turbocharge-langchain-now-guide-to-20x-faster-embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
